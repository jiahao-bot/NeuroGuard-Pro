import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import os
import time
import math

# ==============================================================================
# ğŸ® å®éªŒæ§åˆ¶å° (æŒ‰é¡ºåºè¿è¡Œè¿™ä¸¤ä¸ª)
# ==============================================================================

# ã€ç¬¬ä¸€æ­¥ï¼šè¿è¡Œ Transformerã€‘ -> éªŒè¯æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¦æ¯” LSTM å¼º
# EXP_ID = "Exp5_Transformer_Strict"
# MODEL_TYPE = "Transformer"

# ã€ç¬¬äºŒæ­¥ï¼šè¿è¡Œ GCNã€‘ -> éªŒè¯å›¾ç»“æ„æ˜¯å¦æœ‰æ•ˆ (é¢„æœŸå¼€å§‹çªç ´ CNN çš„ç“¶é¢ˆ)
EXP_ID = "Exp6_GCN_Strict"
MODEL_TYPE = "GCN"

# ==============================================================================

BATCH_SIZE = 64
EPOCHS = 60
PATIENCE = 10
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
SAVE_DIR = os.path.join('../results/', EXP_ID)

if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)


# --- 1. Transformer æ¨¡å‹ ---
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]


class TransformerModel(nn.Module):
    def __init__(self):
        super(TransformerModel, self).__init__()
        # å…ˆç”¨ Conv1d å°† 19 é€šé“æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ï¼ŒåŒæ—¶ç¼©çŸ­æ—¶é—´ç»´åº¦
        # Input: [Batch, 19, 512]
        self.feature_extract = nn.Sequential(
            nn.Conv1d(19, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32), nn.ReLU(),
            nn.MaxPool1d(2),  # 512 -> 256
            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64), nn.ReLU(),
            nn.MaxPool1d(2)  # 256 -> 128
        )

        # Transformer éƒ¨åˆ†
        self.d_model = 64
        self.pos_encoder = PositionalEncoding(d_model=self.d_model)

        # å®šä¹‰ Encoder å±‚
        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=256, dropout=0.3,
                                                   batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 128, 128),
            nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(128, 2)
        )

    def forward(self, x):
        # x: [Batch, 19, 512]
        x = self.feature_extract(x)  # -> [Batch, 64, 128]

        # Transformer éœ€è¦ [Batch, Seq, Feature]
        x = x.permute(0, 2, 1)  # -> [Batch, 128, 64]

        x = self.pos_encoder(x)
        x = self.transformer(x)

        return self.classifier(x)


# --- 2. æ™®é€š GCN æ¨¡å‹ ---
class StandardGCN(nn.Module):
    def __init__(self):
        super(StandardGCN, self).__init__()

        # 1. ç‰¹å¾æå–: å…ˆå¯¹æ¯ä¸ªèŠ‚ç‚¹æå–æ—¶åºç‰¹å¾
        # æˆ‘ä»¬å¸Œæœ›ä¿æŒ 19 ä¸ªèŠ‚ç‚¹ç‹¬ç«‹
        # ä½¿ç”¨ Group Convï¼Œgroups=19ï¼Œè¿™æ · 19 ä¸ªé€šé“äº’ä¸å¹²æ‰°
        self.temporal_conv = nn.Sequential(
            nn.Conv1d(19, 19 * 8, kernel_size=5, padding=2, groups=19),  # å‡ç»´: æ¯ä¸ªèŠ‚ç‚¹8ä¸ªç‰¹å¾
            nn.BatchNorm1d(19 * 8), nn.ReLU(),
            nn.MaxPool1d(4)  # é™é‡‡æ ·
        )

        # 2. å›¾ç»“æ„å­¦ä¹ 
        # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¯å­¦ä¹ é‚»æ¥çŸ©é˜µ
        self.adj = nn.Parameter(torch.rand(19, 19))
        nn.init.xavier_uniform_(self.adj)

        # 3. å›¾å·ç§¯å±‚æƒé‡ (ç‰¹å¾å˜æ¢)
        self.gcn_weight = nn.Linear(8, 16)  # ä» 8 ç‰¹å¾å˜ä¸º 16 ç‰¹å¾

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(19 * 16 * 128, 128),  # å‡è®¾ maxpool åé•¿åº¦ä¸º 128
            nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(128, 2)
        )

    def forward(self, x):
        # x: [Batch, 19, 512]
        B = x.size(0)

        # 1. æ—¶åºæå–
        x = self.temporal_conv(x)  # [B, 19*8, 128]

        # Reshape ä¸º [B, 19, 8, 128] (èŠ‚ç‚¹åˆ†å¼€)
        x = x.view(B, 19, 8, -1)

        # 2. å›¾å·ç§¯: A * X * W
        A = torch.softmax(self.adj, dim=1)  # å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ

        # èšåˆé‚»å±… (A * X)
        # einsum: batch(b), node_i(i), node_j(j), feat(f), time(t)
        # out[b, i, f, t] = sum_j (A[i, j] * x[b, j, f, t])
        support = torch.einsum('ij,bjft->bift', A, x)

        # ç‰¹å¾å˜æ¢ (* W) -> linear ä½œç”¨åœ¨æœ€åä¸€ç»´ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ feat æ”¾åˆ°æœ€å
        support = support.permute(0, 1, 3, 2)  # [B, 19, 128, 8]
        out = self.gcn_weight(support)  # [B, 19, 128, 16]
        out = torch.relu(out)

        return self.classifier(out)


# --- è¾…åŠ©ç±» ---
class EarlyStopping:
    def __init__(self, patience=7):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model, path)
        elif score < self.best_score:
            self.counter += 1
            if self.counter >= self.patience: self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(model, path)
            self.counter = 0

    def save_checkpoint(self, model, path):
        torch.save(model.state_dict(), path)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def run():
    print("=" * 60)
    print(f"ğŸš€ å¯åŠ¨å®éªŒ: {EXP_ID} | æ¨¡å‹: {MODEL_TYPE}")
    print(f"ğŸ’» è®¾å¤‡: {DEVICE} (GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No'})")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    data = np.load('../processed_data/data_19ch.npz')
    X_train, y_train = data['X_train'], data['y_train']
    X_test, y_test = data['X_test'], data['y_test']

    # æ ‡å‡†åŒ–
    mean, std = np.mean(X_train), np.std(X_train)
    X_train = (X_train - mean) / (std + 1e-8)
    X_test = (X_test - mean) / (std + 1e-8)

    train_dl = DataLoader(TensorDataset(torch.FloatTensor(X_train).to(DEVICE), torch.LongTensor(y_train).to(DEVICE)),
                          batch_size=BATCH_SIZE, shuffle=True)
    test_dl = DataLoader(TensorDataset(torch.FloatTensor(X_test).to(DEVICE), torch.LongTensor(y_test).to(DEVICE)),
                         batch_size=BATCH_SIZE, shuffle=False)

    # 2. æ¨¡å‹åˆå§‹åŒ–
    if MODEL_TYPE == 'Transformer':
        model = TransformerModel().to(DEVICE)
    elif MODEL_TYPE == 'GCN':
        model = StandardGCN().to(DEVICE)

    print(f"ğŸ§  å‚æ•°é‡: {count_parameters(model):,}")

    optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Transformer/GCN å»ºè®®ç¨ä½å­¦ä¹ ç‡
    criterion = nn.CrossEntropyLoss()
    early_stopping = EarlyStopping(patience=PATIENCE)

    best_acc = 0.0
    history = {'acc': [], 'loss': []}
    time_history = []

    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    total_start = time.time()

    for epoch in range(EPOCHS):
        start = time.time()
        model.train()
        loss_val = 0
        correct, total = 0, 0
        for x, y in train_dl:
            optimizer.zero_grad()
            out = model(x)
            loss = criterion(out, y)
            loss.backward()
            optimizer.step()
            loss_val += loss.item()
            _, pred = torch.max(out, 1)
            correct += (pred == y).sum().item()
            total += y.size(0)

        train_acc = 100 * correct / total
        avg_loss = loss_val / len(train_dl)

        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for x, y in test_dl:
                out = model(x)
                _, pred = torch.max(out, 1)
                correct += (pred == y).sum().item()
                total += y.size(0)

        test_acc = 100 * correct / total
        e_time = time.time() - start
        time_history.append(e_time)
        history['acc'].append(test_acc)
        history['loss'].append(avg_loss)

        save_msg = ""
        if test_acc > best_acc:
            best_acc = test_acc
            save_msg = "ğŸ†"

        early_stopping(avg_loss, model, os.path.join(SAVE_DIR, 'best_model.pth'))
        print(
            f"Epoch {epoch + 1:02d} | Time: {e_time:.2f}s | Train: {train_acc:.2f}% | Test: {test_acc:.2f}% {save_msg}")

        if early_stopping.early_stop:
            print("ğŸ›‘ æ—©åœè§¦å‘ï¼")
            break

    total_time = time.time() - total_start
    avg_time = np.mean(time_history)
    print(f"\nâœ… å®éªŒç»“æŸ! æœ€ä½³ Acc: {best_acc:.2f}% | å¹³å‡è€—æ—¶: {avg_time:.4f}s")

    # 3. ä¿å­˜å›¾è¡¨å’ŒæŠ¥å‘Š
    plt.figure(figsize=(10, 5))
    plt.plot(history['acc'], label='Test Acc')
    plt.plot(history['loss'], label='Loss')
    plt.title(f'{EXP_ID} Curves')
    plt.legend()
    plt.savefig(os.path.join(SAVE_DIR, 'curve.png'))
    plt.close()

    # Report
    model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'best_model.pth'), weights_only=True))
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for x, y in test_dl:
            out = model(x)
            _, p = torch.max(out, 1)
            preds.extend(p.cpu().numpy())
            labels.extend(y.cpu().numpy())

    with open(os.path.join(SAVE_DIR, 'report.txt'), 'w') as f:
        f.write(f"Experiment: {EXP_ID}\n")
        f.write(f"Best Accuracy: {best_acc:.2f}%\n")
        f.write(f"Avg Time per Epoch: {avg_time:.4f}s\n")
        f.write(f"Params: {count_parameters(model):,}\n\n")
        f.write(classification_report(labels, preds, digits=4))

    cm = confusion_matrix(labels, preds)
    plt.figure()
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.savefig(os.path.join(SAVE_DIR, 'confusion_matrix.png'))
    plt.close()


if __name__ == '__main__':
    run()